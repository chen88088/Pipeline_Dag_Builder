# === dag_modules.py ===
# 專門放 DAG task modules，僅供 Airflow DAG 執行時導入

import requests
import json
import random
import string
from datetime import datetime

# 注意：下列 import 僅會在 Airflow context 中運作
try:
    from airflow.hooks.http_hook import HttpHook
    from confluent_kafka import Producer
except ModuleNotFoundError:
    HttpHook = None
    Producer = None

# === Kafka message function ===
def send_message_to_kafka(message, kafka_conf, topic):
    if Producer is None:
        print("[Warning] Kafka Producer not available (likely outside Airflow runtime).")
        return
    producer = Producer(kafka_conf)
    try:
        producer.produce(topic, value=json.dumps(message))
        producer.flush()
        print("Message sent to Kafka successfully!")
    except Exception as e:
        print(f"Failed to send message to Kafka: {e}")

# === Task: Generate Execution ID ===
def generate_execution_id(ti):
    now_str = datetime.now().strftime("%Y%m%dx%H%M%S")
    rand_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    execution_id = f"{now_str}xx{rand_str}"
    print(f"[Generate_Execution_ID] execution_id = {execution_id}")
    ti.xcom_push(key='execution_id', value=execution_id)

# === Task: Get External Pod Info ===
def get_ml_serving_pod_info(ti, image_name, image_tag, export_port):
    if HttpHook is None:
        raise RuntimeError("HttpHook unavailable outside Airflow")
    http_hook = HttpHook(http_conn_id='controller_connection', method='POST')
    body = {
        "image_name": image_name,
        "image_tag": image_tag,
        "export_port": export_port
    }
    response = http_hook.run('/create_pod', json=body)
    result = response.json()
    ti.xcom_push(key='assigned_service_instance', value=result['pod_name'])
    ti.xcom_push(key='assigned_ip', value=result['pod_service'])
    ti.xcom_push(key='assigned_port', value=export_port)

# === Task: Delete Pod ===
def delete_ml_serving_pod(ti, info_source_task_ids):
    if HttpHook is None:
        raise RuntimeError("HttpHook unavailable outside Airflow")
    pod_name = ti.xcom_pull(key='assigned_service_instance', task_ids=info_source_task_ids)
    http_hook = HttpHook(http_conn_id='controller_connection', method='DELETE')
    response = http_hook.run(f'/delete_pod/{pod_name}')
    print("Pod deleted", response.json())

# === Task: General API Call ===
class ApiCaller:
    def __init__(self, ti, info_source_task_ids, route, body, kafka_conf, kafka_topic):
        self.ti = ti
        self.route = route
        self.body = body
        self.kafka_conf = kafka_conf
        self.kafka_topic = kafka_topic
        self.ip = ti.xcom_pull(key='assigned_ip', task_ids=info_source_task_ids)
        self.port = ti.xcom_pull(key='assigned_port', task_ids=info_source_task_ids)

    def call_api(self):
        url = f"http://{self.ip}:{self.port}/{self.route}"
        try:
            resp = requests.post(url, json=self.body)
            resp.raise_for_status()
            result = resp.json()
            send_message_to_kafka({"TASK_ID": self.route, "TASK_API_RESPOND": result}, self.kafka_conf, self.kafka_topic)
        except Exception as e:
            raise Exception(f"API call failed: {self.route} - {e}")

def call_api_task(ti, info_source_task_ids, route, body, kafka_conf, kafka_topic):
    ApiCaller(ti, info_source_task_ids, route, body, kafka_conf, kafka_topic).call_api()

# === Optional: used in /deploy-dag API service to prepare input ===
def build_body_from_config(task, dag_id: str) -> dict:
    body = {
        "DAG_ID": dag_id,
        "EXECUTION_ID": None,
        "MODEL_NAME": "",
        "MODEL_VERSION": "",
        "DEPLOYER_NAME": "AutoGenerated",
        "DEPLOYER_EMAIL": "noreply@example.com",
        "PIPELINE_CONFIG": {
            "params": {param["key"]: param["value"] for param in task.config.get("config_params", [])}
        },
        "TASK_STAGE_TYPE": "Training"
    }

    # 條件加入
    if task.config.get("dataset_name"):
        body["DATASET_NAME"] = task.config["dataset_name"]
    if task.config.get("dataset_version"):
        body["DATASET_VERSION"] = task.config["dataset_version"]
    if task.config.get("dvc_repo"):
        body["DATASET_DVCFILE_REPO"] = task.config["dvc_repo"]
    if task.config.get("code_repo_url"):
        body["CODE_REPO_URL"] = {"Training": task.config["code_repo_url"]}
    if task.config.get("image_name"):
        body["IMAGE_NAME"] = {"Training": task.config["image_name"]}
    if task.config.get("script_list"):
        body["EXECUTION_SCRIPTS"] = {"Training": task.config["script_list"]}
    if task.config.get("script_name"):
        body["UPLOAD_MLFLOW_SCRIPT"] = {"Training": task.config["script_name"]}

    return body